<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>4-6 | Home</title><meta name=keywords content="NLP"><meta name=description content="lexical semantics 词义分析
词义 语义
几个语言学术语 词义：词的内容
概念义——客观 色彩义——主观 义素（Sememe）：最小词义单位，又叫语义特征 、义原等。
义位（glosseme）：能独立运用的最小词义单位（义素）
义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征 义位是由义素合成的 义素分析法：
确定对比分析的义位，通常是同一语义场内的一些义位 寻找义位之间的共性特征和区别性特征 将寻找出的各种义素用结构式描述出来 提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？
基于符号的方法 从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义
基于基本词 基于一个基本词(概念)集合来表示其他所有词的语义
基本词是义素 (语义特征\义原)
构建基本词集合：
专家定义 知网（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系 义原定义过程：将词的关系归结到其义原间的关系 首先对汉字（单纯词）进行考察和分析，获取一些义原 然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充 这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语 例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣) 很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式 缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化 自动发现 近年来出现了一些基于语料来自动获取基本词的研究 提出新义原还没有 基于词间关系 词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系
上下义位关系(Hyponymy)——名词、动词等都可以有
上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)
哺乳动物&mdash;>动物
下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)
动物&mdash;>哺乳动物
全体-成员关系(Ensemble-Member)
从全体到成员关系 (Has-Member) 从成员到全体 整体-部分关系(Whole-part)
从整体到部分: Part Meronym(Has-Part) 从部分到整体:Part Holonym(Part-Of) 同义关系(Synonymy)
两个词(基于音、义位)之间的关系 同音词 (Homonyms) 同形(同音)异义词 (Homographs) 同形异音异义词(Heteronyms) 近义词 (Synonyms): 具有相同或相近义位的不同词 反义词(Antonyms): 互补 分级 关系 自反 换喻 (Metonyms) 用一个对象来指称另一个对象 用一个对象的属性或某个侧面来指称另一个对象 用一个属性来指称一个对象 区分："><meta name=author content="Chan"><link rel=canonical href=/posts/nlp/04-06lexcial-semantics/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="4-6"><meta property="og:description" content="lexical semantics 词义分析
词义 语义
几个语言学术语 词义：词的内容
概念义——客观 色彩义——主观 义素（Sememe）：最小词义单位，又叫语义特征 、义原等。
义位（glosseme）：能独立运用的最小词义单位（义素）
义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征 义位是由义素合成的 义素分析法：
确定对比分析的义位，通常是同一语义场内的一些义位 寻找义位之间的共性特征和区别性特征 将寻找出的各种义素用结构式描述出来 提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？
基于符号的方法 从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义
基于基本词 基于一个基本词(概念)集合来表示其他所有词的语义
基本词是义素 (语义特征\义原)
构建基本词集合：
专家定义 知网（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系 义原定义过程：将词的关系归结到其义原间的关系 首先对汉字（单纯词）进行考察和分析，获取一些义原 然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充 这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语 例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣) 很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式 缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化 自动发现 近年来出现了一些基于语料来自动获取基本词的研究 提出新义原还没有 基于词间关系 词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系
上下义位关系(Hyponymy)——名词、动词等都可以有
上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)
哺乳动物&mdash;>动物
下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)
动物&mdash;>哺乳动物
全体-成员关系(Ensemble-Member)
从全体到成员关系 (Has-Member) 从成员到全体 整体-部分关系(Whole-part)
从整体到部分: Part Meronym(Has-Part) 从部分到整体:Part Holonym(Part-Of) 同义关系(Synonymy)
两个词(基于音、义位)之间的关系 同音词 (Homonyms) 同形(同音)异义词 (Homographs) 同形异音异义词(Heteronyms) 近义词 (Synonyms): 具有相同或相近义位的不同词 反义词(Antonyms): 互补 分级 关系 自反 换喻 (Metonyms) 用一个对象来指称另一个对象 用一个对象的属性或某个侧面来指称另一个对象 用一个属性来指称一个对象 区分："><meta property="og:type" content="article"><meta property="og:url" content="/posts/nlp/04-06lexcial-semantics/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-08T11:25:09+00:00"><meta property="article:modified_time" content="2022-04-08T11:25:09+00:00"><meta property="og:site_name" content="Chancellor16's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="4-6"><meta name=twitter:description content="lexical semantics 词义分析
词义 语义
几个语言学术语 词义：词的内容
概念义——客观 色彩义——主观 义素（Sememe）：最小词义单位，又叫语义特征 、义原等。
义位（glosseme）：能独立运用的最小词义单位（义素）
义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征 义位是由义素合成的 义素分析法：
确定对比分析的义位，通常是同一语义场内的一些义位 寻找义位之间的共性特征和区别性特征 将寻找出的各种义素用结构式描述出来 提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？
基于符号的方法 从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义
基于基本词 基于一个基本词(概念)集合来表示其他所有词的语义
基本词是义素 (语义特征\义原)
构建基本词集合：
专家定义 知网（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系 义原定义过程：将词的关系归结到其义原间的关系 首先对汉字（单纯词）进行考察和分析，获取一些义原 然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充 这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语 例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣) 很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式 缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化 自动发现 近年来出现了一些基于语料来自动获取基本词的研究 提出新义原还没有 基于词间关系 词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系
上下义位关系(Hyponymy)——名词、动词等都可以有
上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)
哺乳动物&mdash;>动物
下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)
动物&mdash;>哺乳动物
全体-成员关系(Ensemble-Member)
从全体到成员关系 (Has-Member) 从成员到全体 整体-部分关系(Whole-part)
从整体到部分: Part Meronym(Has-Part) 从部分到整体:Part Holonym(Part-Of) 同义关系(Synonymy)
两个词(基于音、义位)之间的关系 同音词 (Homonyms) 同形(同音)异义词 (Homographs) 同形异音异义词(Heteronyms) 近义词 (Synonyms): 具有相同或相近义位的不同词 反义词(Antonyms): 互补 分级 关系 自反 换喻 (Metonyms) 用一个对象来指称另一个对象 用一个对象的属性或某个侧面来指称另一个对象 用一个属性来指称一个对象 区分："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":3,"name":"4-6","item":"/posts/nlp/04-06lexcial-semantics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"4-6","name":"4-6","description":"lexical semantics 词义分析\n词义 语义\n几个语言学术语 词义：词的内容\n概念义——客观 色彩义——主观 义素（Sememe）：最小词义单位，又叫语义特征 、义原等。\n义位（glosseme）：能独立运用的最小词义单位（义素）\n义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征 义位是由义素合成的 义素分析法：\n确定对比分析的义位，通常是同一语义场内的一些义位 寻找义位之间的共性特征和区别性特征 将寻找出的各种义素用结构式描述出来 提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？\n基于符号的方法 从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义\n基于基本词 基于一个基本词(概念)集合来表示其他所有词的语义\n基本词是义素 (语义特征\\义原)\n构建基本词集合：\n专家定义 知网（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系 义原定义过程：将词的关系归结到其义原间的关系 首先对汉字（单纯词）进行考察和分析，获取一些义原 然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充 这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语 例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣) 很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式 缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化 自动发现 近年来出现了一些基于语料来自动获取基本词的研究 提出新义原还没有 基于词间关系 词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系\n上下义位关系(Hyponymy)——名词、动词等都可以有\n上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)\n哺乳动物\u0026mdash;\u0026gt;动物\n下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)\n动物\u0026mdash;\u0026gt;哺乳动物\n全体-成员关系(Ensemble-Member)\n从全体到成员关系 (Has-Member) 从成员到全体 整体-部分关系(Whole-part)\n从整体到部分: Part Meronym(Has-Part) 从部分到整体:Part Holonym(Part-Of) 同义关系(Synonymy)\n两个词(基于音、义位)之间的关系 同音词 (Homonyms) 同形(同音)异义词 (Homographs) 同形异音异义词(Heteronyms) 近义词 (Synonyms): 具有相同或相近义位的不同词 反义词(Antonyms): 互补 分级 关系 自反 换喻 (Metonyms) 用一个对象来指称另一个对象 用一个对象的属性或某个侧面来指称另一个对象 用一个属性来指称一个对象 区分：","keywords":["NLP"],"articleBody":"lexical semantics 词义分析\n词义 语义\n几个语言学术语 词义：词的内容\n概念义——客观 色彩义——主观 义素（Sememe）：最小词义单位，又叫语义特征 、义原等。\n义位（glosseme）：能独立运用的最小词义单位（义素）\n义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征 义位是由义素合成的 义素分析法：\n确定对比分析的义位，通常是同一语义场内的一些义位 寻找义位之间的共性特征和区别性特征 将寻找出的各种义素用结构式描述出来 提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？\n基于符号的方法 从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义\n基于基本词 基于一个基本词(概念)集合来表示其他所有词的语义\n基本词是义素 (语义特征\\义原)\n构建基本词集合：\n专家定义 知网（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系 义原定义过程：将词的关系归结到其义原间的关系 首先对汉字（单纯词）进行考察和分析，获取一些义原 然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充 这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语 例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣) 很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式 缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化 自动发现 近年来出现了一些基于语料来自动获取基本词的研究 提出新义原还没有 基于词间关系 词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系\n上下义位关系(Hyponymy)——名词、动词等都可以有\n上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)\n哺乳动物—\u003e动物\n下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)\n动物—\u003e哺乳动物\n全体-成员关系(Ensemble-Member)\n从全体到成员关系 (Has-Member) 从成员到全体 整体-部分关系(Whole-part)\n从整体到部分: Part Meronym(Has-Part) 从部分到整体:Part Holonym(Part-Of) 同义关系(Synonymy)\n两个词(基于音、义位)之间的关系 同音词 (Homonyms) 同形(同音)异义词 (Homographs) 同形异音异义词(Heteronyms) 近义词 (Synonyms): 具有相同或相近义位的不同词 反义词(Antonyms): 互补 分级 关系 自反 换喻 (Metonyms) 用一个对象来指称另一个对象 用一个对象的属性或某个侧面来指称另一个对象 用一个属性来指称一个对象 区分：\n多义词(Polysemy) vs. 同形异义词(Homographs) 系统性(Systematically) vs. 偶然性(Occasionally) 不同义位间有系统性关联 vs. 没有关联 词义消歧（WSD） Word Sense Disambiguation:\n两类具体任务：\nLexical Sample：从词典里为给定上下文的某个特定词选择正确的义项 All-words WSD：从词典里为文中所有目标多义词选择正确的义项 1998年起开始的SENSEVAL评测最初只有英语的Lexical Sample任务，后续逐步增加了其他语种的任务以及All-words WSD任务，2007年后改为SemEval，增加了更多丰富的评测项目\n词间（义位）相似性 比近义词更一般的词关系\n两个词相似有很多层面：语义相似、用法相似、形态相似等，这里主要从语义的角度\n如：sim(猫,狗)\u003e sim(猫,桌子)、sim(站,坐)\u003e sim(站 vs 看待)\n词间相似性具有很多重要语言使用和理解价值\n如何度量相似性？是一个重要的语言处理问题\n词间（义位）相关性 语义场：这些词更可能共同出现在一个特定的场景中，具有相关性\n词间相关性对于消歧等任务都具有重要价值\n相关性和相似性不同：\n相关不一定相似 ·············································································相似一定相关：？ 利用词间关系定义词义的例子 同义词词林(中文)\nWordNet(英文)：同义词集合(Synset)表示一个义项\n如：\npublish, print publish, write publish, bring out, put out, issue, release 结合的方法 FrameNet：框架(frame)\n上述基于词间关系表达词义的问题：\n哪些词间关系 词间关系多样、复杂、主观性高：近义词粒度 符号计算不直接：在目前的计算框架下不可直接计算，需要再设计量化的算法 例如：基于WordNet的词语相似性，两个词之间的路径长度或进一步的改进度量 基于某种更简单的关系、定量化\n基于向量的方法 基于统计的高维向量及其降维 分布式(distributional)词(义)表示 基本思想：基于目标词上下文的词来定义目标词（同现关系：直接可观）\n词表 W = (w~1~ , … , w~v~)\n目标词 w~t~∈W的向量表示 x=(x~1~, … , x~v~)\n其中每个 x~i~ 如何确定：词 w~t~ 与 w~i~ 的同现情况（同现：在一个上下文中同时出现）\n上下文：给定语料C和窗口K：\n布尔型：w~i~ 是否出现在 w~t~ 的窗口内 0/1 频次型：w~i~ 出现在 w~t~ 的窗口内的次数 f 例1：\n例2：\n优点：\n每个词对应一个向量，可以直接计算词间语义关系 每一维是有意义的(某个词) 缺点：\n维数过高(维数=词表大小)，计算复杂度高 词间独立无关：同义词等在不同维 矩阵定义词向量的优点：在一定程度上缓解了one-hot向量相似度为0的问题，但依旧没有解决数据系数和维度灾难的问题\n降维高维向量 既然基于共现矩阵得到的离散词向量存在着高维和稀疏性的问题，一个自然而然的解决思路是对原始词向量进行降维，从而得到一个稠密的连续词向量。\n选择某些维度（特征选择）：tfidf…后面讲文本编码时会再提到 压缩到特定维(特征抽取)：隐性语义索引LSI(Latent Semantic Index）也叫LSA(latent semantic analysis)潜层语义分析… LSI的核心是对频率矩阵进行SVD(Singular Value Decomposition)\n首先要像上面那样获取共现矩阵，然后对矩阵进行SVD，即可得到降维后的词向量\nSVD分解 【学长小课堂】什么是奇异值分解SVD–SVD如何分解时空矩阵_哔哩哔哩_bilibili\n机器学习值SVD（二）_知然xu的博客-CSDN博客\n奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园 (cnblogs.com)\n专业详细推导：Singular Value Decomposition (SVD) tutorial (mit.edu)\nSVD分解和特征分解类似，都是将一个矩阵 M 分解成三个矩阵 USV^T^ 相乘，但是特征分解要求是方阵才可以分解，SVD不要求必须是方阵。\nSVD分解：\n从几何的角度：M是一个线性变换，将原始坐标系转换到新的坐标系，V 和 U 分别代表原始和新的坐标系的标准正交基，S 表示奇异值矩阵 从意义的角度：SVD分解分解成三个矩阵相乘，转变形式可以分成两个矩阵相乘，中间量代表了一种含义，在分解出来之后才能依据具体情况知道它的意义。 SVD算法求解：如何求 USV^T^：\n求 M^T^M 的特征向量得到 V 求 MM^T^ 的特征向量得到 U 求 M^T^M 或 MM^T^ 的特征值，开平方得到奇异值，将奇异值从大到小排列成对角矩阵，得到 S 利用SVD降维高维向量 CS224N自然语言处理1：导论+词向量 | 在那不遥远的地方 (zhang-each.github.io)\nNLP — 文本分类（基于SVD的隐语意分析（LSA））_zsffuture的博客-CSDN博客\n过程：得到共现矩阵，进行svd分解，得到USV~T~，选择降维后的维度d，截取得到降维后的 tU 和 tS，则tU×tS即为低维向量\n例子：见课件\n问题：\n降维后每一维的具体物理含义不如原始的那么清楚 高频词会对其他词产生过大的影响 REF：\n首先，统计获得共生矩阵X。X是一个|V|×|V| 大小的矩阵，Xij表示在所有语料中，词汇表V中第i个词和第j个词同时出现的词数，|V|为词汇表的大小。对X做矩阵分解（如奇异值分解SVD）得到正交矩阵U，对U进行归一化得到矩阵（正交矩阵是实数特殊化的酉矩阵）\nSVD方法还是存在一定的缺陷的，不管用什么方式生成矩阵X，都会面临这样几个问题：\n计算量较大，对于m×n的矩阵，SVD的时间复杂度是O(mn^2^)，因此维数高了以后计算量会暴增 生成的矩阵要插入新单词或者新文档的内容时比较麻烦，矩阵的规模很容易变化 生成的矩阵往往比较稀疏，因为大部分单词和单词或者单词和文档之间可能没什么关系 面对这些问题，可以采取的解决措施有：\n不考虑一些功能性的单词比如a，an，the，it等等 使用一个倾斜的窗口，也就是在生成共生矩阵的时候考虑单词相距的距离 基于预测的低维向量 Word2Vec 什么是Word2Vec和Embeddings？\nWord2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定中心词来预测上下文，而CBOW是给定上下文来预测中心词。\nEmbedding (词嵌入) 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。\n我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。\n术语：\nDistributed Representation(分布式表示) Word Vector (词向量) Word Embedding (词嵌入) Continuous Space Representation (连续空间表示） 2个模型\nCBOW Skip-gram 2种训练方法\n层次softmax 负采样 ——\u003e 交叉出4种方式\nCBOW + Hierarchical SoftMax CBOW(Continuous Bag-Of-Words)：通过上下文环境来预测当前词\n对于目标词wt 及其上下文C(wt)：P(wt |C(wt))，想要使P(wt |C(wt))极大 (至少P(wt |C(wt)\u003eP(非wt |C(wt))）\n但是 非wt 是什么？\n一个直接的方案是构建V类的softmax，所有其他wt 均为非wt ：\n但是模型复杂度高，如何更高效：\n层次softmax，将一个V分类分解为一系列的二分到达某个词\n如何构造：基于语料中的词频，从根节点出发，频率高的具有较短的路径\n如何计算某个叶子节点的概率：每个二分类(y={0,1})都采用logistic回归，极大化路径总似然，即到达正确词的路径的概率\n如何训练参数：可以采用随机梯度下降法\n层次softmax问题：\n树的结构影响大 树的训练复杂性高 能否更简洁：直接用非w解决而不是用所有w？\nSkip-Gram + Negative Sampling 基于：语义相近的词出现的位置相近\nNLP基础-词嵌入-第4期-揭开词嵌入中skip-gram的神秘面纱|Demystifying Neural Network in Skip-Gram Lang_哔哩哔哩_bilibili\nNLP代码实战-01-gensim训练自己的skip-gram词向量-基于凡人修仙传的语料_哔哩哔哩_bilibili\n详解 Word2vec 之 Skip-gram 模型_奋起的小渣渣的博客-CSDN博客\nCS224n笔记2 词的向量表示：word2vec-码农场 (hankcs.com)\nSkip-gram模型：预测当前词的上下文词（相反：CBOW：通过上下文环境来预测当前词）\nSkip-gram模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Skip-gram中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。（上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。）\n所以，第一步，建立模型（Fake Task）；第二步，训练模型，获得参数，即词向量。\n首先来看如何建立模型？\n建立模型/Fake Task 训练模型 采用下采样(subsampling)技术降低无效语料规模：\n向量选择：\n算法的计算复杂性：正比于语料规模|C|, O(|C|)\n词向量评估 词表示评估：\n外部任务：用于下游任务（文本分类，语言理解，机器翻译。。。） 内部任务（直接，结果立现）：词相似度任务，词类别任务 词相似度任务：数据集\n汉语：PKU500、CWE297,CWE240 英语：WordSim353、RW、MEN 德语：ZG222、Gur250 词类比任务：给出三个找第四个\n句法类比 语义类比 数据集： [Mikolov2013NAACL] 创建了句法类比数据集，含8000条 semantic analogy数据采用的是SemEval-2012 Task 2, Measuring Relation Similarity，由[Jurgens2012SemEval]创建。 问题与发展 如何利用全局统计信息来预测，而不是局部上下文？\nPennington等[Pennington2014EMNLP]认为比同现概率更有效的是同现概率的比，基于此提出了GloVe(Global Vectors）\n低频词的词向量学习？\n多义词的词向量学习？\n词向量应用 总结 ","wordCount":"392","inLanguage":"en","datePublished":"2022-04-08T11:25:09Z","dateModified":"2022-04-08T11:25:09Z","author":{"@type":"Person","name":"Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"/posts/nlp/04-06lexcial-semantics/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href accesskey=h title="Cheyne16's Blog (Alt + H)"><img src=/apple-touch-icon.png alt aria-label=logo height=35>Cheyne16's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/categories/ title=categories><span>categories</span></a></li><li><a href=/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div><h1 class=post-title>4-6</h1><div class=post-meta><span title='2022-04-08 11:25:09 +0000 UTC'>April 8, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;392 words&nbsp;·&nbsp;Chan&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/NLP/04-06Lexcial%20Semantics.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>lexical semantics 词义分析</p><p>词义 语义</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220408150219853.png alt=image-20220408150219853></p><h1 id=几个语言学术语>几个语言学术语<a hidden class=anchor aria-hidden=true href=#几个语言学术语>#</a></h1><p>词义：词的内容</p><ul><li>概念义——客观</li><li>色彩义——主观</li></ul><p><strong>义素</strong>（Sememe）：最小词义单位，又叫语义特征 、义原等。</p><p><strong>义位</strong>（glosseme）：能独立运用的最小词义单位（义素）</p><ul><li>义素是可以通过比较分析一组相关词语的义位(义素分析法)而得到的词义的区别特征</li><li>义位是由义素合成的</li></ul><p>义素分析法：</p><ul><li>确定对比分析的义位，通常是同一语义场内的一些义位</li><li>寻找义位之间的共性特征和区别性特征</li><li>将寻找出的各种义素用结构式描述出来</li></ul><p>提供词义的词典，目前都是给人使用的，机器不能用，如何为机器提供词义呢？</p><h1 id=基于符号的方法>基于符号的方法<a hidden class=anchor aria-hidden=true href=#基于符号的方法>#</a></h1><p>从借鉴词典的词义定义开始：用有限的基本词或者用与其他词(不一定是基本词)的关系来表达词义</p><h2 id=基于基本词>基于基本词<a hidden class=anchor aria-hidden=true href=#基于基本词>#</a></h2><p>基于一个基本词(概念)集合来表示其他所有词的语义</p><p>基本词是义素 (语义特征\义原)</p><p>构建基本词集合：</p><ul><li>专家定义<ul><li><strong>知网</strong>（HowNet）是一个基于义原的常识知识库，基于义原揭示概念与概念之间以及概念所具有的属性之间的关系</li><li>义原定义过程：将词的关系归结到其<strong>义原</strong>间的关系<ul><li>首先对汉字（单纯词）进行考察和分析，获取一些义原</li><li>然后用这些义原作为标注集去标注多音节的词，当发现这些义原不满足要求时，便进行调整或扩充</li><li>这样最终形成了2000多个义原的标注集以及由它们标注的10万个中文/英文词或短语</li><li>例：打的意思有（1）买：打酱油（2）辫编：打毛衣。那么判断”打手套“中的打是什么意思只需要比较d(手套，酱油)和d(手套，毛衣)</li></ul></li><li>很多词典是基于专家确定的一些基本词而构建出来的，如Oxford学生词典：专家确定2000基础词，其他词均由基础词定义，因此从这些词典中可以获取方式</li><li>缺点：主观性，不一致，费时费力；新词不断出现、词义逐渐变化</li></ul></li><li>自动发现<ul><li>近年来出现了一些基于语料来自动获取基本词的研究</li><li>提出新义原还没有</li></ul></li></ul><h2 id=基于词间关系>基于词间关系<a hidden class=anchor aria-hidden=true href=#基于词间关系>#</a></h2><p>词间关系，主要是词义之间的关系，当词有多个义位时，看其中一义位间的关系。所以更准确地说两个词之间的关系是指这两个词的某两个义位之间的关系</p><ol><li><p>上下义位关系(Hyponymy)——名词、动词等都可以有</p><ol><li><p>上位(Superordinate上位词)：从特殊概念到一般概念 (IS-A)</p><p>哺乳动物&mdash;>动物</p></li><li><p>下位(Subordinate下位词)： 从一般概念到特殊概念 (Include)</p><p>动物&mdash;>哺乳动物</p></li></ol></li><li><p>全体-成员关系(Ensemble-Member)</p><ol><li>从全体到成员关系 (Has-Member)</li><li>从成员到全体</li></ol></li><li><p>整体-部分关系(Whole-part)</p><ol><li>从整体到部分: Part Meronym(Has-Part)</li><li>从部分到整体:Part Holonym(Part-Of)</li></ol></li><li><p>同义关系(Synonymy)</p></li></ol><h3 id=两个词基于音义位之间的关系>两个词(基于音、义位)之间的关系<a hidden class=anchor aria-hidden=true href=#两个词基于音义位之间的关系>#</a></h3><ul><li>同音词 (Homonyms)</li><li>同形(同音)异义词 (Homographs)</li><li>同形异音异义词(Heteronyms)</li><li>近义词 (Synonyms): 具有相同或相近义位的不同词</li><li>反义词(Antonyms):<ul><li>互补</li><li>分级</li><li>关系</li><li>自反</li></ul></li><li>换喻 (Metonyms)<ul><li>用一个对象来指称另一个对象</li><li>用一个对象的属性或某个侧面来指称另一个对象</li><li>用一个属性来指称一个对象</li></ul></li></ul><p>区分：</p><ul><li>多义词(Polysemy) vs. 同形异义词(Homographs)</li><li>系统性(Systematically) vs. 偶然性(Occasionally)</li><li>不同义位间有系统性关联 vs. 没有关联</li></ul><h3 id=词义消歧wsd>词义消歧（WSD）<a hidden class=anchor aria-hidden=true href=#词义消歧wsd>#</a></h3><p>Word Sense Disambiguation:</p><p>两类具体任务：</p><ul><li>Lexical Sample：从词典里为给定上下文的<strong>某个特定词</strong>选择正确的义项</li><li>All-words WSD：从词典里为文中<strong>所有目标多义词</strong>选择正确的义项</li></ul><p>1998年起开始的SENSEVAL评测最初只有英语的Lexical Sample任务，后续逐步增加了其他语种的任务以及All-words WSD任务，2007年后改为SemEval，增加了更多丰富的评测项目</p><h3 id=词间义位相似性>词间（义位）相似性<a hidden class=anchor aria-hidden=true href=#词间义位相似性>#</a></h3><p>比近义词更一般的词关系</p><p>两个词相似有很多层面：语义相似、用法相似、形态相似等，这里主要从语义的角度</p><p>如：sim(猫,狗)> sim(猫,桌子)、sim(站,坐)> sim(站 vs 看待)</p><p>词间相似性具有很多重要语言使用和理解价值</p><p>如何度量相似性？是一个重要的语言处理问题</p><h3 id=词间义位相关性>词间（义位）相关性<a hidden class=anchor aria-hidden=true href=#词间义位相关性>#</a></h3><p>语义场：这些词更可能共同出现在一个特定的场景中，具有相关性</p><p>词间相关性对于消歧等任务都具有重要价值</p><p>相关性和相似性不同：</p><ul><li>相关不一定相似</li><li>·············································································相似一定相关：？</li></ul><h3 id=利用词间关系定义词义的例子>利用词间关系定义词义的例子<a hidden class=anchor aria-hidden=true href=#利用词间关系定义词义的例子>#</a></h3><ol><li><p>同义词词林(中文)</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220408213803764.png alt=image-20220408213803764></p></li><li><p>WordNet(英文)：同义词集合(Synset)表示一个义项</p><p>如：</p><ul><li>publish, print</li><li>publish, write</li><li>publish, bring out, put out, issue, release</li></ul></li></ol><h2 id=结合的方法>结合的方法<a hidden class=anchor aria-hidden=true href=#结合的方法>#</a></h2><p>FrameNet：框架(frame)</p><p>上述基于词间关系表达词义的问题：</p><ul><li>哪些词间关系</li><li>词间关系多样、复杂、主观性高：近义词粒度</li><li>符号计算不直接：在目前的计算框架下不可直接计算，需要再设计量化的算法</li><li>例如：基于WordNet的词语相似性，两个词之间的路径长度或进一步的改进度量</li></ul><p><strong>基于某种更简单的关系、定量化</strong></p><h1 id=基于向量的方法>基于向量的方法<a hidden class=anchor aria-hidden=true href=#基于向量的方法>#</a></h1><h2 id=基于统计的高维向量及其降维>基于统计的高维向量及其降维<a hidden class=anchor aria-hidden=true href=#基于统计的高维向量及其降维>#</a></h2><h3 id=分布式distributional词义表示>分布式(distributional)词(义)表示<a hidden class=anchor aria-hidden=true href=#分布式distributional词义表示>#</a></h3><ul><li><p>基本思想：基于目标词上下文的词来定义目标词（同现关系：直接可观）</p></li><li><p>词表 W = (w~1~ , &mldr; , w~v~)</p></li><li><p>目标词 w~t~∈W的向量表示 x=(x~1~, &mldr; , x~v~)</p></li><li><p>其中每个 x~i~ 如何确定：词 w~t~ 与 w~i~ 的同现情况（同现：在一个上下文中同时出现）</p></li><li><p>上下文：给定语料C和窗口K：</p><ul><li>布尔型：w~i~ 是否出现在 w~t~ 的窗口内 0/1</li><li>频次型：w~i~ 出现在 w~t~ 的窗口内的次数 f</li></ul></li><li><p>例1：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220409091634105.png alt=image-20220409091634105></p></li><li><p>例2：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220409090239705.png alt="image-20220409090239705 "></p></li><li><p>优点：</p><ul><li>每个词对应一个向量，可以直接计算词间语义关系</li><li>每一维是有意义的(某个词)</li></ul></li><li><p>缺点：</p><ul><li>维数过高(维数=词表大小)，计算复杂度高</li><li>词间独立无关：同义词等在不同维</li></ul></li><li><p>矩阵定义词向量的优点：在一定程度上缓解了one-hot向量相似度为0的问题，但依旧没有解决数据系数和维度灾难的问题</p></li></ul><h3 id=降维高维向量>降维高维向量<a hidden class=anchor aria-hidden=true href=#降维高维向量>#</a></h3><p>既然基于共现矩阵得到的离散词向量存在着<strong>高维</strong>和<strong>稀疏性</strong>的问题，一个自然而然的解决思路是对原始词向量进行<strong>降维</strong>，从而得到一个稠密的连续词向量。</p><ul><li>选择某些维度（特征选择）：tfidf…后面讲文本编码时会再提到</li><li>压缩到特定维(特征抽取)：隐性语义索引LSI(Latent Semantic Index）也叫LSA(latent semantic analysis)潜层语义分析&mldr;<ul><li><p>LSI的核心是对频率矩阵进行SVD(Singular Value Decomposition)</p><p>首先要像上面那样获取共现矩阵，然后对矩阵进行SVD，即可得到降维后的词向量</p></li></ul></li></ul><h4 id=svd分解>SVD分解<a hidden class=anchor aria-hidden=true href=#svd分解>#</a></h4><blockquote><p><a href="https://www.bilibili.com/video/BV16A411T7zX?spm_id_from=333.337.search-card.all.click">【学长小课堂】什么是奇异值分解SVD&ndash;SVD如何分解时空矩阵_哔哩哔哩_bilibili</a></p><p><a href=https://blog.csdn.net/m0_37565948/article/details/84990043>机器学习值SVD（二）_知然xu的博客-CSDN博客</a></p><p><a href=https://www.cnblogs.com/pinard/p/6251584.html>奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园 (cnblogs.com)</a></p><p>专业详细推导：<a href=http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm>Singular Value Decomposition (SVD) tutorial (mit.edu)</a></p></blockquote><p>SVD分解和特征分解类似，都是将一个矩阵 M 分解成三个矩阵 USV^T^ 相乘，但是特征分解要求是方阵才可以分解，SVD不要求必须是方阵。</p><p>SVD分解：</p><ul><li>从几何的角度：M是一个线性变换，将原始坐标系转换到新的坐标系，V 和 U 分别代表原始和新的坐标系的标准正交基，S 表示奇异值矩阵</li><li>从意义的角度：SVD分解分解成三个矩阵相乘，转变形式可以分成两个矩阵相乘，中间量代表了一种含义，在分解出来之后才能依据具体情况知道它的意义。</li></ul><p>SVD算法求解：如何求 USV^T^：</p><ul><li>求 M^T^M 的特征向量得到 V</li><li>求 MM^T^ 的特征向量得到 U</li><li>求 M^T^M 或 MM^T^ 的特征值，开平方得到奇异值，将奇异值从大到小排列成对角矩阵，得到 S</li></ul><h4 id=利用svd降维高维向量>利用SVD降维高维向量<a hidden class=anchor aria-hidden=true href=#利用svd降维高维向量>#</a></h4><blockquote><p><a href=https://zhang-each.github.io/2021/08/28/nlp1/>CS224N自然语言处理1：导论+词向量 | 在那不遥远的地方 (zhang-each.github.io)</a></p><p><a href=https://blog.csdn.net/weixin_42398658/article/details/85088130>NLP &mdash; 文本分类（基于SVD的隐语意分析（LSA））_zsffuture的博客-CSDN博客</a></p></blockquote><p>过程：得到共现矩阵，进行svd分解，得到USV~T~，选择降维后的维度d，截取得到降维后的 tU 和 tS，则tU×tS即为低维向量</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220414181804482.png alt=image-20220414181804482></p><p>例子：见课件</p><p>问题：</p><ul><li>降维后每一维的具体物理含义不如原始的那么清楚</li><li>高频词会对其他词产生过大的影响</li></ul><p>REF：</p><blockquote><p>首先，统计获得共生矩阵X。X是一个|V|×|V| 大小的矩阵，Xij表示在所有语料中，词汇表<code>V</code>中第i个词和第j个词同时出现的词数，|V|为词汇表的大小。对X做矩阵分解（如奇异值分解SVD）得到<strong>正交矩阵U</strong>，对U进行<strong>归一化</strong>得到矩阵（<strong>正交矩阵</strong>是实数特殊化的酉<strong>矩阵</strong>）</p><p>SVD方法还是存在一定的缺陷的，不管用什么方式生成矩阵X，都会面临这样几个问题：</p><ul><li>计算量较大，对于m×n的矩阵，SVD的时间复杂度是O(mn^2^)，因此维数高了以后计算量会暴增</li><li>生成的矩阵要插入新单词或者新文档的内容时比较麻烦，矩阵的规模很容易变化</li><li>生成的矩阵往往比较稀疏，因为大部分单词和单词或者单词和文档之间可能没什么关系</li></ul><p>面对这些问题，可以采取的解决措施有：</p><ul><li>不考虑一些功能性的单词比如a，an，the，it等等</li><li>使用一个倾斜的窗口，也就是在生成共生矩阵的时候考虑单词相距的距离</li></ul></blockquote><h2 id=基于预测的低维向量-word2vec>基于预测的低维向量 Word2Vec<a hidden class=anchor aria-hidden=true href=#基于预测的低维向量-word2vec>#</a></h2><p>什么是<a href="https://so.csdn.net/so/search?q=Word2Vec&spm=1001.2101.3001.7020">Word2Vec</a>和Embeddings？</p><ul><li><p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定中心词来预测上下文，而CBOW是给定上下文来预测中心词。</p></li><li><p>Embedding (词嵌入) 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p></li><li><p>我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。</p></li></ul><p>术语：</p><ul><li>Distributed Representation(分布式表示)</li><li>Word Vector (词向量)</li><li>Word Embedding (词嵌入)</li><li>Continuous Space Representation (连续空间表示）</li></ul><p>2个模型</p><ul><li>CBOW</li><li>Skip-gram</li></ul><p>2种训练方法</p><ul><li>层次softmax</li><li>负采样</li></ul><p>&mdash;&mdash;> 交叉出4种方式</p><h3 id=cbow--hierarchical-softmax>CBOW + Hierarchical SoftMax<a hidden class=anchor aria-hidden=true href=#cbow--hierarchical-softmax>#</a></h3><p>CBOW(Continuous Bag-Of-Words)：通过上下文环境来预测当前词</p><p>对于目标词wt 及其上下文C(wt)：P(wt |C(wt))，想要使P(wt |C(wt))极大 (至少P(wt |C(wt)>P(非wt |C(wt))）</p><p>但是 非wt 是什么？</p><p>一个直接的方案是构建V类的softmax，所有其他wt 均为非wt ：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220409101954546.png alt=image-20220409101954546></p><p>但是模型复杂度高，如何更高效：</p><p>层次softmax，将一个V分类分解为一系列的二分到达某个词</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220409102036786.png alt=image-20220409102036786></p><p>如何构造：基于语料中的词频，从根节点出发，频率高的具有较短的路径</p><p>如何计算某个叶子节点的概率：每个二分类(y={0,1})都采用logistic回归，极大化路径总似然，即到达正确词的路径的概率</p><p>如何训练参数：可以采用随机梯度下降法</p><p>层次softmax问题：</p><ul><li>树的结构影响大</li><li>树的训练复杂性高</li></ul><p>能否更简洁：直接用非w解决而不是用所有w？</p><h3 id=skip-gram--negative-sampling>Skip-Gram + Negative Sampling<a hidden class=anchor aria-hidden=true href=#skip-gram--negative-sampling>#</a></h3><p>基于：语义相近的词出现的位置相近</p><blockquote><p><a href="https://www.bilibili.com/video/BV1gq4y1B7XC/?spm_id_from=trigger_reload">NLP基础-词嵌入-第4期-揭开词嵌入中skip-gram的神秘面纱|Demystifying Neural Network in Skip-Gram Lang_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1vY4y1h7TK?spm_id_from=333.337.search-card.all.click">NLP代码实战-01-gensim训练自己的skip-gram词向量-基于凡人修仙传的语料_哔哩哔哩_bilibili</a></p><p><a href=https://blog.csdn.net/qq_38587650/article/details/120637940>详解 Word2vec 之 Skip-gram 模型_奋起的小渣渣的博客-CSDN博客</a></p><p><a href=http://www.hankcs.com/nlp/word-vector-representations-word2vec.html>CS224n笔记2 词的向量表示：word2vec-码农场 (hankcs.com)</a></p></blockquote><p>Skip-gram模型：预测当前词的上下文词（相反：CBOW：通过上下文环境来预测当前词）</p><p><img loading=lazy src=https://raw.githubusercontent.com/Cheyne16/ImageHub/main/image-20220411194708876.png alt=image-20220411194708876></p><p>Skip-gram模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是<strong>这个模型通过训练数据所学得的参数</strong>，例如隐层的权重矩阵——后面我们将会看到这些<strong>权重</strong>在Skip-gram中实际上就是我们试图去学习的“<strong>word vectors</strong>”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。（上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。）</p><p>所以，第一步，建立模型（Fake Task）；第二步，训练模型，获得参数，即词向量。</p><p>首先来看如何建立模型？</p><h4 id=建立模型fake-task>建立模型/Fake Task<a hidden class=anchor aria-hidden=true href=#建立模型fake-task>#</a></h4><h4 id=训练模型>训练模型<a hidden class=anchor aria-hidden=true href=#训练模型>#</a></h4><p>采用下采样(subsampling)技术降低无效语料规模：</p><p>向量选择：</p><p>算法的计算复杂性：正比于语料规模|C|, O(|C|)</p><h3 id=词向量评估>词向量评估<a hidden class=anchor aria-hidden=true href=#词向量评估>#</a></h3><p>词表示评估：</p><ul><li>外部任务：用于下游任务（文本分类，语言理解，机器翻译。。。）</li><li>内部任务（直接，结果立现）：词相似度任务，词类别任务</li></ul><p>词相似度任务：数据集</p><ul><li>汉语：PKU500、CWE297,CWE240</li><li>英语：WordSim353、RW、MEN</li><li>德语：ZG222、Gur250</li></ul><p>词类比任务：给出三个找第四个</p><ul><li>句法类比</li><li>语义类比</li><li>数据集：<ul><li>[Mikolov2013NAACL] 创建了句法类比数据集，含8000条</li><li>semantic analogy数据采用的是SemEval-2012 Task 2, Measuring Relation Similarity，由[Jurgens2012SemEval]创建。</li></ul></li></ul><h3 id=问题与发展>问题与发展<a hidden class=anchor aria-hidden=true href=#问题与发展>#</a></h3><ol><li><p>如何利用<strong>全局统计信息</strong>来预测，而不是局部上下文？</p><p>Pennington等[Pennington2014EMNLP]认为比同现概率更有效的是<strong>同现概率的比</strong>，基于此提出了<strong>GloVe</strong>(Global Vectors）</p></li><li><p>低频词的词向量学习？</p></li><li><p>多义词的词向量学习？</p></li></ol><h3 id=词向量应用>词向量应用<a hidden class=anchor aria-hidden=true href=#词向量应用>#</a></h3><h1 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h1></div><footer class=post-footer><ul class=post-tags><li><a href=/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=/posts/nlp/02english-lexical-morphology/><span class=title>« Prev</span><br><span>2</span></a>
<a class=next href=/posts/nlp/chapter3-chinese-lexical-morphology/><span class=title>Next »</span><br><span>Chapter3 - Chinese Lexical Morphology</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on twitter" href="https://twitter.com/intent/tweet/?text=4-6&url=%2fposts%2fnlp%2f04-06lexcial-semantics%2f&hashtags=NLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2fposts%2fnlp%2f04-06lexcial-semantics%2f&title=4-6&summary=4-6&source=%2fposts%2fnlp%2f04-06lexcial-semantics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on reddit" href="https://reddit.com/submit?url=%2fposts%2fnlp%2f04-06lexcial-semantics%2f&title=4-6"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on facebook" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fnlp%2f04-06lexcial-semantics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on whatsapp" href="https://api.whatsapp.com/send?text=4-6%20-%20%2fposts%2fnlp%2f04-06lexcial-semantics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 4-6 on telegram" href="https://telegram.me/share/url?text=4-6&url=%2fposts%2fnlp%2f04-06lexcial-semantics%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href>Home</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>