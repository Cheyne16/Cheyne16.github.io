<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>决策树 on Home</title>
    <link>/tags/%E5%86%B3%E7%AD%96%E6%A0%91/</link>
    <description>Recent content in 决策树 on Home</description>
    <image>
      <title>Home</title>
      <url>%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator><atom:link href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>决策树</title>
      <link>/posts/ml/%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/ml/%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>ID3 entropy
conditional entropy
information gain
find min(conditional entropy) to get max(information gain)
Occam&amp;rsquo;s razor: Entities are not to be multiplied beyond necessity (law of parsimony).
C4.5 information-gain ratio
choose max(information-gain ratio)
for continuous-valued attributes: find the threshold.
for unknown attribute values: ignore the unknown then calc; divide the sample by weight.
CART-CLASSIFICATION TREE Gini In=dex [p~k~ for k~th~ class]=
Gini index for binary classification problem [p for 1~th~ class]</description>
    </item>
    
  </channel>
</rss>
